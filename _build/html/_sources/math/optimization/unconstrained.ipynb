{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncontraied optimization\n",
    "\n",
    "minimize $f(x)$\n",
    "- $f$ convex, twice continuously differentiable (hence $dom f$ open)\n",
    "- we assume optimal value $p^*=\\inf_x f(x)$ is attained (and finite)\n",
    "solving the uncontrained optimization = find the solution to:\n",
    "$\\nabla f(x^*)=0$ $(n equations)\n",
    "\n",
    "usually need iterative algorithms:\n",
    "- produce a sequence of $x^{(k)}\\in dom(f), k=1,2,\\dots$ with $f(x^{(k)}) \\to \\min f(x)$ (i.e., $\\nabla f(x^{(k)}) $) \n",
    "- \n",
    "\n",
    "Descent methods\n",
    "- start with $x^{(0)}$\n",
    "- update $x^{(k+1)} = x^{(k)} + t^{(k)} \\Delta x^{(k)}$ s.t $f(x^{(k+1)}< f(x^{(k)})$\n",
    "- other notations: $x^+=x+ t \\Delta x$, $x:=x+t\\Delta x$\n",
    "- $\\Delta x$ is the step, or search direction; $t$ is the step size, or step length\n",
    "- from convexity, $f(x^+) < f(x)$ implies $\\nabla f(x)^T \\Delta x < 0$ (i.e., $\\Delta x$ is a descent direction)\n",
    "\n",
    "General descent method.\n",
    "- given a starting point $x\\in dom(f)$.\n",
    "- repeat \n",
    "1. determie a descent direction $\\Delta x$.\n",
    "2. line search\n",
    "3. Update: $x:=x+t \\Delta $\n",
    "\n",
    "Determine a descent direction\n",
    "- one approach is gradient descent $\\Delta x = -\\nabla f(x)$\n",
    "- choose a step size\n",
    "1. line search methods: exact line search $t=argmin_{t>0} f(x+t \\Delta x)$\n",
    "2. backtracking \n",
    "\n",
    "find the steepest descent direction - steepest descent method\n",
    "normalized steepest descent direction\n",
    "$\\Delta x_{nsd} = argmin \\{ \\nabla f(x)^T v \\mid ||v|| = 1 \\}$\n",
    "$\\Delta x = -\\nabla f(x)$ is the steepest descent direction with respect to Euclidean norm\n",
    "\n",
    "so performance of steepest descent similar to graident descent\n",
    "\n",
    "### Newton's method\n",
    "Newton's direction $\\Delta x_{nt} = - \\nabla^2 f(x)^{-1} \\nabla f(x)$\n",
    "\n",
    "One interpretation\n",
    "- $x+\\Delta x_{nt}$ minimizes second order approximation\n",
    "$f(x+v)=f(x) + \\nabla f(x)^T v + \\frac{1}{2} v^T \\nabla^2 f(x) v$\n",
    "\n",
    "`intuition`: $f$ twice differentiable, so this quadratic model is very accurate when $x$ is near $x^*$.\n",
    "\n",
    "Newton's descrement (stopping criteria):\n",
    "$\\lambda(x) = (\\nabla f(x)^T \\nabla^2 f(x)^{(-1)} \\nabla f(x))^{1/2}$ a measure of the proximity of $x$ to $x^*$\n",
    "- gives an estimate of $f(x) - p^*$, using quadratic approximation $\\hat{f}$:\n",
    "$f(x) - \\inf_v \\hat{f} (x+v) = \\frac{1}{2} \\lambda(x)^2$\n",
    "- directional derivative in the Newton direction: $\\nabla f(x)^T \\Delta x_{nt} = -\\lambda(x)^2$\n",
    "\n",
    "(damped or guarded) Newton's method:\n",
    "\n",
    "given a starting point , tolerance $\\epsilon$\n",
    "repeat \n",
    "1. computer the Newton step and decrement.\n",
    "2. stopping criterion.\n",
    "3. linea search\n",
    "4. update\n",
    "\n",
    "- damped Newton phase: $t<1$\n",
    "- quadratically convergent phase: $t=1$\n",
    "- pros: convergence is rapid in general and quadratic near $x^*$\n",
    "- cons: compute and store $\\nabla^2 f(x)$ and $\\nabla^2f(x)^{-1}$ (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing one lecture [todo]\n",
    "\n",
    "### infeasible start Newton method\n",
    "\n",
    "A generalization that deals with infeasible initial points and iterates\n",
    "- let x be a point that we do not assume to be feasible\n",
    "- find $x+\\Delta x_{nt}$ that solves the second-order approximation\n",
    "$\\min \\hat{f}(x+v)=f(x)+\\nabla f(x)^T v + (1/2)v^T\\nabla^2 f(x) v$ s.t. $A(x+v) =b$\n",
    "\n",
    "primal Newton step, dual Newton step\n",
    "\n",
    "primal-dual Newton step (an alternative way to derive)\n",
    "- write optimality condition as $r(y) = 0$, where \n",
    "$y=(x,v)$, $r(y) = (\\nabla f(x) + A^T v, Ax - b )$ can be understood as $(r_{dual} (x,v) , r_{pri} (x,v)$\n",
    "- linearizing $r(y) = 0$ gives $r(y+ \\Delta y) \\approx r(y) + D r(y) \\Delta y = 0$\n",
    "`first-order approx of first-order approx = second-order`\n",
    "\n",
    "\n",
    "- given starting point $x\\in dom f, v$, tolerance $\\epsilon > 0$, $\\alpha\\in (0, 1/2)$, $\\beta \\in (0,1)$.\n",
    "- repeat\n",
    "1. Compute primal aand dual Newton steps $\\Delta x_{nt}$, $\\Delta v_{nt}$\n",
    "2. Backtracking line search on $||r||_2$. $t:=1$.\n",
    "while $||r(x+t\\Delta x_{nt}, v+t \\Delta v_{nt})||_2 > (1-\\alpha t) || r(x,v)||_2$, $t:=\\beta t$\n",
    "3. Update. $x:=x+t\\Delta x_{nt}$, $v:=v+ t\\Delta v_{nt}$.\n",
    "- until $Ax = b$ and $||r(x,v) || \\le \\epsilon$.\n",
    "\n",
    "$r(y) + D r(y) \\Delta y = 0$\n",
    "\n",
    "- not a decent method: \n",
    "- the norm of $r$ decreases in the Newton's direction:\n",
    "- if $t=1$, the next iterate will be feasible, and all the following iterates will be feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for Equality Constrained Optimization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e8ef2f9fcac0817bca9a7ca376f64f20b4df5ea3bf7af756a50bda7d3557ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
