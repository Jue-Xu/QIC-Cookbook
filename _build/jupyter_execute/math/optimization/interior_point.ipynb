{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18897cbd",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc4ba4",
   "metadata": {},
   "source": [
    "## Interior-Point methods\n",
    "\n",
    "### basic ideas\n",
    "\n",
    "move inequality constraint to objective function via `indicator functions`\n",
    "- $\\min f_0(x) + \\sum_{i=1}^m I_- (f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "where $I_-(u)=0$ if $u\\le 0$, $I_-(u)=\\infty$ otherwise (indicator function of $R_-$).\n",
    "\n",
    "#### Logarithmic barrier function\n",
    "\n",
    "approximation via `logarithmic barrier`: fix some $t>0$\n",
    "- $\\min f_0(x) - (1/t) \\sum_{i=1}^m \\log (-f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "$\\phi(x)=-\\sum_{i=1}^m \\log(-f_i(x))$, $dom \\phi = \\{x\\mid f_1(x) \\le 0, \\dots, f_m(x)<0 \\}$\n",
    "- convex (follows from composition rules)\n",
    "- twice continuously differentiable, with `gradient Hessian`\n",
    "\n",
    "becomes\n",
    "- $\\min f_0(x) - (1/t)\\sum \\log(-f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "- difficult to minimize using Newton's method (from a random starting point) when t is large\n",
    "\n",
    "because Hessian varies rapidly near boundary of feasibility set\n",
    "\n",
    "- can be circumvented by solving a sequence of problems with increasing $t$\n",
    "\n",
    "startnig each Newton minmization from the solution to the problem with previous $t$\n",
    "\n",
    "#### Central path\n",
    "\n",
    "- central paths: $\\{ x^*(t) \\mid t>0 \\}$\n",
    "- $x^*(t)$: central points\n",
    "\n",
    "example: central path for an LP\n",
    "- $\\min c^T x$\n",
    "- s.t. $a_i^T x \\le b_i$, $i=1,\\dots,6$\n",
    "\n",
    "hyperplane $c^T x = c^T x^*(t)$ is tangent to level curve of $\\phi$ through $x^*(t)$\n",
    "\n",
    "* take the central path through interior of the feasible set\n",
    "\n",
    "`analytic center` of a set of convex inequalities and linear equations\n",
    "$f_i(x) \\le 0$, $i=1,\\dots,m$, $Fx=g$\n",
    "is defined as the optimal point of \n",
    "- $\\min -\\sum_i^m \\log(-f_i(x))$\n",
    "- s.t. $Fx=g$\n",
    "analytic center of linear inequalities $a_i^T x\\le b_i$, $i=1,\\dots,m$\n",
    "\n",
    "$x_{ac}$ is minimizer of $\\phi(x) = -\\sum_i^m \\log(b_i-a_i^T x)$\n",
    "\n",
    "### Barrier method (one interior-point method)\n",
    "\n",
    "- given strictly feasible $x$, $t:=t^{(0)}>0$, $\\mu>1$, tolerance $\\epsilon > 0$.\n",
    "repeat \n",
    "1. Centering step. Compute $x^*(t)$ by minimizing $t f_0 + \\phi$, subject to $Ax=b$\n",
    "2. Update $x:=x^*(t)$\n",
    "3. Stopping criterion. quit if \n",
    "4. increase $t$. $t:=\\mu t$\n",
    "\n",
    "choice of $\\mu$ involves a trade-off: large $\\mu$ means fewer outer interations, more inner (Newton) iterations; typical values: $\\mu=10-20$\n",
    "(for more practical choices of parameters, pp.570, textbook)\n",
    "\n",
    "#### dual points from central path\n",
    "\n",
    "every $x^*(t)$ corresponds to a dual feasible point (`of the original inequality constrained problem`)\n",
    "$\\lambda_i^*(t) = 1/(-tf_i(x^*(t)))$ and $\\nu^*(t) = w/t$\n",
    "\n",
    "verification:\n",
    "$x^*(t)$ solves \n",
    "- $\\min t f_0(x) + \\phi(x)$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "implies \n",
    "- $Ax^* = b$, $f_i(x^*) < 0$, $i=1, \\dots, m$\n",
    "- $\\exists w$, $t\\nabla f_0(x^*) + \\sum \\frac{1}{-f_i(x^*)} \\nabla f_i(x^*) + A^T w=0$\n",
    "\n",
    "implies \n",
    "- $x^*(t)$ minimizes the Lagrangian (of the `original problem`)\n",
    "- $L(x,\\lambda^*(t), \\nu^*(t))=f_0(x) + \\sum \\lambda_i^*(t) f_i(x) + \\nu^*(t)^T (Ax-b)$ \n",
    "- at: $\\lambda_i^*(t) = 1/(-t f_i(x^*(t)))$ and $\\nu^*(t) = w/t$\n",
    "\n",
    "Duality gap $m/t$: \n",
    "$f_0(x^*(t)) \\ge p^* \\ge d^* \\ge g(\\lambda^*(t), \\nu^*(t)) = L(x^*(t), \\lambda^*(t),\\nu^*(t))=f_0(x^*(t))-m/t$\n",
    "\n",
    "#### Interpretation via KKT condition\n",
    "$x=x^*(t)$, $\\lambda=\\lambda^*(t)$, $\\nu=\\nu^*(t)$ satsify\n",
    "1. primal constraints: $f_i(x)\\le 0$, $i=1,\\dots,m$, $Ax=b$\n",
    "2. dual constraints: $\\lambda\\succeq 0$\n",
    "3. approximate complementary slackness: \n",
    "4. \n",
    "\n",
    "#### Convergnce \n",
    "The number of steps to converge within tolerance $\\epsilon$:\n",
    "\n",
    "plus the initial centering step (to compute $x^*(t^{(0)})$)\n",
    "\n",
    "Example: geometric program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd730a3",
   "metadata": {},
   "source": [
    "#### Basic phase I method\n",
    "\n",
    "- $\\min (over x,s) s$\n",
    "- s.t. $f_i(x)\\le s, i=1,\\dots,m$\n",
    "- $Ax=b$\n",
    "\n",
    "* if $x$, $s$ feasible, with $s<0$, then $x$ is strictly feasible for (1)\n",
    "* if optimal value $p^*$ of (2) is positive, then problem (1) is feasible\n",
    "* if $p^* =0$  and attained, then problem (1) is feasible (but not strictly);\n",
    "if $p^* =0$ and not attained, then problem (1) is infeasible\n",
    "\n",
    "Sum of infeasibilities phase I method\n",
    "- $\\min 1^T s$\n",
    "- s.t. $s\\succeq 0$, $f_i(x)\\le s_i$, $i=1,\\dots,m$\n",
    "- $Ax=b$\n",
    "\n",
    "* The optimal value of (3) is zero and achieved iff problem (1) is feasible\n",
    "\n",
    "#### Phase I via Newton method\n",
    "\n",
    "#### Primal-dual interior-point method\n",
    "Update both primal and dual variables in the Newton method\n",
    "to solve the modified KKT conditions\n",
    "\n",
    "- no distinction between inner and outer iterations\n",
    "- the primal iterates are not necessarily feasible \n",
    "- $\\hat{\\eta}(x,\\lambda) = -f(x)^T \\lambda$: surrogate duality gap\n",
    "- backtracking line search based on the norm of the residuals \n",
    "\n",
    "#### backtracking line search\n",
    "based on the norm of the residuals, modified to ensure $f(x)\\prec 0$, $\\lambda \\succeq 0$.\n",
    "\n",
    "compute the largest positive step length that does not exceed one and gives $\\lambda^+\\succeq 0$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f8e99",
   "metadata": {},
   "source": [
    "## Subgradient Method\n",
    "\n",
    "$g$ is a subgradient of $f$ (not necessarily convex) at $x$ if\n",
    "$f(y)\\ge f(x) + g^T (y-x) \\forall y$\n",
    "\n",
    "- if $f$ is convex and differentiable, $\\nabla f(x)$\n",
    "- \n",
    "- \n",
    "\n",
    "````{prf:example} subgradient\n",
    ":label: subgradient\n",
    "\n",
    "$f=\\max\\{f_1,f_2\\}$ with $f_1$, $f_2$ convex and differentiable\n",
    "\n",
    "- $f_1(x_0)> f_2(x_0)$: unique subgradient $g=\\nabla f_1(x_0)$\n",
    "- $f_2(x_0)> f_1(x_0)$: unique subgradient $g=\\nabla f_2(x_0)$\n",
    "- $f_1(x_0) = f_2(x_0)$: subgradient form a line segment $[\\nablaf_1(x_0),\\nabla f_2(x_0)]$\n",
    "````\n",
    "\n",
    "### Subdifferential\n",
    "\n",
    "- set of all subgradients of $f$ at $x$ is called the subdifferential of $f$ at $x$, denoted $\\partial f(x)$\n",
    "- $\\partial f(x)$ is a closed convex set (can be empty)\n",
    "\n",
    "if $f$ is convex,\n",
    "- $\\partial f(x)$ is nonempty, for $x\\in dom f$\n",
    "- $\\partial f(x) = \\{ \\nabla f(x) \\}$, if $f$ is differentiable at $x$\n",
    "- if $\\partial f(x) = \\{ g \\}$, ...\n",
    "\n",
    "recall for $f$ convex, differentiable $f(x^*)=\\inf_x f(x) \\Longleftrightarrow 0 = \\nabla f(x^*)$\n",
    "\n",
    "### Optimality condition for unconstrained problem\n",
    "\n",
    "- $\\min f_0(x)$\n",
    "- s.t. $f_i(x) \\le 0, i\\in [m$\n",
    "\n",
    "we assume \n",
    "- $f_i$ convex, defined on $R^n$ (hence subdiffereentiable)\n",
    "- strict feaisbility (Slater's condition)\n",
    "\n",
    "$x^*$ is primal optimal ($\\lambda^*$ is dual optimal) iff\n",
    "- $f_i(x^*)\\le 0$, $\\lambda^*_1 \\le 0$\n",
    "- ..\n",
    "- ..\n",
    "\n",
    "the algorithm is similar to ...\n",
    "\n",
    "#### Step size\n",
    "step size rules (step sizes are fixed before algorithm execution)\n",
    "- constant stetp size \n",
    "- constant step length\n",
    "- square summable\n",
    "- \n",
    "\n",
    "#### Convergence\n",
    "assumption\n",
    "- $f^*= \\inf_x f(x) > -\\infty$ , with $f(x^*)=f^*$\n",
    "- $||g||_2 \\le G \\forall g\\in \\partial f$\n",
    "- $R\\ge ||x^{(1)} - x^*||_2$\n",
    "\n",
    "convergence results: define $\\bar{f}=\\lim_{k\\to \\infty} f_{best}^{(k)}$\n",
    "- constant step size: $\\bar{h} - f^* \\le G^2 \\alpha/2$, i.e., converges to $G^2\\alpha/2$-suboptimal (converges to $f^*$ if $\\alpha$ small enough)\n",
    "- constant step length: $G\\gamma /2$-suboptimal\n",
    "\n",
    "````{prf:example}\n",
    "minimize $f(x) = \\max_{i=1,\\dots,m} (a_i^T x + b_i)$\n",
    "````\n",
    "\n",
    "### Subgradient method for constrained problems\n",
    "\n",
    "#### Projection\n",
    "projection: $s=argmin_{s\\in C} ||x-s||_s$\n",
    "\n",
    "example: linear equality constrained problem\n",
    "- $\\min f(x)$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "projection of $z$ onto $\\{ x\\mid Ax=b \\}$ is \n",
    "```{math}\n",
    "P(z) = z- A^T(AA^T)^{-1} (Az-b) \n",
    "= (I - A^T (AA^T)^{-1} A) z + A^T(AA^T)^{-1}b)\n",
    "```\n",
    "\n",
    "````{prf:example} Linear equality constrained problem\n",
    "- minimize $||x||_1$\n",
    "- s.t. $Ax = b$\n",
    "\n",
    "subgradient of objective is $g= sign(x)$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7bf2ca",
   "metadata": {},
   "source": [
    "### Projected subgradient method for dual problem\n",
    "\n",
    "(convex) primal: (Slater's condition holds)\n",
    "- minimize $f_0(x)$\n",
    "- s.t. $f_i(x) \\le 0 $\n",
    "\n",
    "solve dual problem \n",
    "- maximize $g(\\lambda)$\n",
    "- s.t. $\\lambda \\succeq 0$\n",
    "\n",
    "via projected subgradient method:\n",
    "$\\lambda^{(k+1)} =... $\n",
    "\n",
    "Given a starting point $\\lambda$....\n",
    "Repeat\n",
    "\n",
    "iterpretation: \n",
    "- $\\lambda_i$ is price for resource $f_i(x)$\n",
    "- price update $\\lambda_i^{(k+1)}=\\qty(\\lambda_i^{(k)}+\\alpha_k f_i(x^{(k)}))_+$\n",
    "- increase price $\\lambda_i$ if resource $i$ is over-utilized (i.e., $f_i(x) >0$)\n",
    "- decrease price $\\lambda_i$ if ... is under-utilizedo\n",
    "- not negative\n",
    "\n",
    "````{prf:example} \n",
    "minimize strictly convex quadratic $(P\\succ 0)$ over unit box:\n",
    "\n",
    "- minimize $(1/2)x^T P x -q^T x$\n",
    "- s.t. $x_i^2 \\le 1$\n",
    "\n",
    "- $L(x,\\lambda) = (1/2)x^T (P+diag(2\\lambda))x - q^T x - 1^T \\lambda$\n",
    "- $x^*(\\lambda) = (P+diag(2\\lambda))^{-1} q$\n",
    "- projected \n",
    "````\n",
    "\n",
    "#### Cutting plane oracle\n",
    "Oracle: a device that can answer question for us; we make no assumption on how an answer is found by the device\n",
    "\n",
    "Cutting plane oracle (also called separation oracle), once queried at $x$, either\n",
    "- asserts $x\\in X$ ($X\\subseteq R^n$)\n",
    "- or return a separating hyperplane between $x$ and $X$: $a\\neq 0$,\n",
    "$a^T z \\le b$ for $z\\in X$, $a^T x \\ge b$\n",
    "$(a,b)$ called a cutting-plane, or cut, since it eliminates the halfspace $\\{ z\\mid a^T z > b \\}$ from our search for a point in $X$\n",
    "\n",
    "- neutral cut\n",
    "- deep cut\n",
    "\n",
    "unconstrained optimization\n",
    "- $f_0:R^n\\to R$, convex; $x^*$ is the optimal solution; $X$ is the set of optimal points\n",
    "- Given $x$, find $g\\in \\partial f_0 (x)$\n",
    "- $x^*$ belongs to the halfspace: $\\{ z\\mid g^T(z-x) \\le 0 \\},\\forall x$\n",
    "- $g^T(z-x)\\le 0$ defines a `cutting plane` at $x (a=g,b=g^Tx)$\n",
    "\n",
    "#### inequality constrained optimization\n",
    "- If $x$ is feasible, we have a (neural) objective cut\n",
    "$g_0^T(z-x)\\le 0$, $g_0\\in\\partial f_0(x)$\n",
    "- If $x$ is not feasible, e.g., $f_j(x)>0$, we have a (deep) feasibility cut\n",
    "$f_j(x) + g_j^T (z-x) \\le 0$, $g_j \\in \\partial f_j(x)$\n",
    "\n",
    "basic (conceptual) cutting-plane/localzation algorithm\n",
    "\n",
    "#### choosing the center point (specific localization methods)\n",
    "- center of gravity\n",
    "- analytic centering\n",
    "- Chebyshev center\n",
    "- Center of the maximum volume epplisoid (MVE)\n",
    "\n",
    "#### Bisection method on R\n",
    "- minimize convex $f:R\\to R$\n",
    "- $P_k$ is an interval\n",
    "- obvious choice for query point: $x^{(k+1)}:=midpoint(P_k)$\n",
    "\n",
    "Bisection algorithm for one-dimensional search\n",
    "\n",
    "given an initial interval $[l,u]$ known to contain $x^*$; a required tolerance $r>0$\n",
    "repeat \n",
    "    - $x:=(l+u)/2$.\n",
    "    - query the oracle at $x$.\n",
    "    - if the oracle determines that $x^*\\le x, u:=x$\n",
    "    - if the oracle determines that $x^*\\ge x, l:=x$\n",
    "until $u-l\\e 2r$\n",
    "\n",
    "for differentiable $f$: evaluate $f'(x)$ if $f'(x)<0$, $l:=x$; else $u:=x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed7539",
   "metadata": {},
   "source": [
    "### Ellipsoid method (for unconstrained convex problems)\n",
    "Idea: Use ellipsoids to approximate polyhedron, find $x^*$ in an ellipsoid instead of a polyhedron\n",
    "\n",
    "Algorithm sketch:\n",
    "1. at iteration $k$ we know  \n",
    "2. set\n",
    "3. hence we know\n",
    "\n",
    "Basic ellipsoid algorithm (for unconstrained convex problems)\n",
    "\n",
    "- given ellipsoid $E(x,P)$ containing\n",
    "- $\\sqrt{g^T P g}$\n",
    "\n",
    "For inequaltiy constrained problems\n",
    "- If $x^{(k)}$ feasible, update ellipsoid with `objective` cut\n",
    "\n",
    "a (deep) cut\n",
    "\n",
    "- If $x^{(k)}$ infeasible, update ellipsoid with `feasible` cut\n",
    "\n",
    "minimum volume ellipsoid containing ellipsoid intersected with halfspace\n",
    "\n",
    "where $\\tilde{g} = \\frac{g}{\\sqrt{g^T P g}}$. \n",
    "\n",
    "Stopping criteria\n",
    "- if $x^{(k)}$ is feasible and $\\sqrt{g_0^{(k)T} P^{(k)} g_0^{(k)} }\\le \\epsilon$\n",
    "- if $f_j(x^{(k)}) - \\sqrt{g_j^{(k)T} P^{(k)} g_j^{(k)} } > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3143d3",
   "metadata": {},
   "source": [
    "## Decomposition methods\n",
    "\n",
    "Break a problem into smaller ones and solve each of the smaller ones separably, either in parallel or sequentially\n",
    "\n",
    "### Separable problems\n",
    "- minimize $f_1(x_1)+ f_2(x_2)$\n",
    "- s.t. $x_1\\in C_1$, $x_2\\in C_2$\n",
    "\n",
    "we can solve for $x_1 and $x_2$ separately (in parallel)\n",
    "\n",
    "consider uncontrained problem, \n",
    "- minimize $f(x) = f_1(x_1,y) + f_2(x_2,y)$, $x=(x_1,x_2,y)$\n",
    "\n",
    "$y$ is the `complicating variable` or `coupling variable`; when it is fixed the problem is separable in $x_1$ and $x_2$\n",
    "\n",
    "### Primal decomposition method\n",
    "fix $y$ and define\n",
    "- subproblem1: $\\min_{x_1} f_1(x_1,y)$\n",
    "- subproblem2: $\\min_{x_2} f_1(x_2,y)$\n",
    "\n",
    "with optimal values $\\phi_1(y)$ and $\\phi_2(y)$\n",
    "\n",
    "master problme $\\min_{y} \\phi_1(y) + \\phi_2(y)$\n",
    "\n",
    "- can solve master problem using \n",
    "1. bisection (if $y$ is scalar)\n",
    "2 gradient or Newton method (if $\\phi_i$ differentiable)\n",
    "3 subgradient, cutting-plane, or ellipsoid method\n",
    "\n",
    "- each \n",
    "\n",
    "Algorithm sketch\n",
    "\n",
    "- Given $y^{(0)}$, $k=0$;\n",
    "- Repeat \n",
    "- solve two subproblems to derive $x_1$, $x_2$\n",
    "- update $y^{(k)}$ to $y^{(k+1)}$ based on the algorithm to solve the master problem\n",
    "\n",
    "### Dual decomposition method\n",
    "\n",
    "- Step 1: \n",
    "- Step 2: form dual problem\n",
    "````{math} \n",
    "L(x_1, y_1, x_2, y_2) = f_1(x_1,y_1) + f_2(x_2,y_2) + \\nu^T (y_1 - y_2)\n",
    "\n",
    "separable; can minimize over $(x_1,y_1)$ and $(x_2,y_2)$ separately\n",
    "- $g_1(\\mu) = \\inf_{x_1,y_1} (f_1(x_1,y_1)+\\nu^T y_1) = -f_1^*(0,-\\nu)$\n",
    "- $g_2(\\mu) = \\inf_{x_2,y_2} (f_2(x_2,y_2)-\\nu^T y_2) = -f_2^*(0,\\nu)$\n",
    "\n",
    "dual problem is: maximize $g(\\nu) = g_1(\\nu) + g_2(\\nu)$\n",
    "\n",
    "- compuing $g_i(\\nu)$ are the dual subproblems\n",
    "- can be done in parallel\n",
    "- a subgradient of $-g$ is $y_2-y_1$ (from solutions of sub)\n",
    "````\n",
    "\n",
    "Dual decomposition algorithm\n",
    "\n",
    "(using subgradient algorithm for master)\n",
    "\n",
    "- repeat\n",
    "1. solve the dual subproblems (in parallel)\n",
    "    find\n",
    "2. Update dual variables (prices). \n",
    "$\\nu:=\\nu - \\alpha_k(y_2-y_1)$\n",
    "\n",
    "- step lenght $\\alpha_k$ can be chosen in standard ways\n",
    "- at each step we have a lowe bound $g(\\nu)$ on $p^*$\n",
    "- iterates are generally infeasible, i.e., $y_1\\neq y_2$\n",
    "\n",
    "Finding feasible iterate\n",
    "- reasonable guess of feasible point from $(x_1,y_1)$, $(x_2,y_2)$:\n",
    "\n",
    "- a better feasible point:\n",
    "\n",
    "### Problems with complicating constraints\n",
    "- $\\min f_1(x_1) + f_2(x_2)$\n",
    "- s.t. $x_1\\in C_1$, $x_2\\in C_2$, $h_1(x_1)+h_2(x_2)\\preceq 0$\n",
    "\n",
    "- $f_i, h_i, C_i$ convex\n",
    "- \n",
    "- can interpret coupling constaints as `limits on resources` shared between two subproblems\n",
    "\n",
    "fix $t\\in R^p$ and define\n",
    "- $t$ is the quantity of resources allocated to first subproblem ($-t$ is allocated to second subproblem) \n",
    "- master problem: minimize $\\phi_1(t) + \\phi_2(t)$ (optimal values of subproblems) over $t$\n",
    "- subproblems can be solve separately (in parallel) when $t$ is fixed\n",
    "\n",
    "Lagrangian relaxation and subgradient method\n",
    "- $\\alpha_k$ is an appropriate step size\n",
    "- iterates need not be feasible\n",
    "- can again construct feasible primal variables using projection\n",
    "\n",
    "````{prf:example}\n",
    "Problem setup\n",
    "- $n$ flows, with fixed routes, in a network with $m$ links\n",
    "- variable $f_i\\ge 0 $ denotes the rate of flow $j$\n",
    "- flow utility is $U_j: R\\to R$, strictly concave, increasing\n",
    "- traffic $t_i$ on link $i$ is the sum of flows passing through it\n",
    "- $t=Rf$, where $R$ is the routing matrix\n",
    "$R_{ij} = 1$ flow j passes over link $i$, 0 otherwise\n",
    "- link capacity constraint: $t\\preceq c$\n",
    "\n",
    "Rate control problem: \n",
    "- maximize $U(f) = \\sum_j^n U_j(f_j)$\n",
    "- s.t. $Rf \\preceq c$\n",
    "\n",
    "- convex problem\n",
    "- dual decomposition gives decentralized method\n",
    "\n",
    "Lagrangian: $L(f,\\lambda) = -\\sum_j^n U_j(f_j) + \\lambda^T (Rf - c)$\n",
    "\n",
    "Lagrangian dual \n",
    "````{math}\n",
    "g(\\lambda) \n",
    "= \\inf_f(\\sum_j^n -U_j(f_j) + \\lambda^T (Rf-c))\n",
    "= -\\lambda^T c + \\sum_j^n \\inf_{f_j} (-U_j(f_j)+(r_j^T \\lambda) f_j) \n",
    "= -\\lambda&^T c - \\sum_j^n(-U_j)^* (-r_j^T \\lambda)\n",
    "````\n",
    "- dual problem:  $\\max -\\lambda^T c - \\sum_{j=1}^n (-U_j)^* (-r_j^T \\lambda)$\n",
    "- s.t. $\\lambda \\succeq 0$\n",
    "\n",
    "a subgradient of $-g$ is given by $c-R\\bar{f}$, where $\\bar{f}_j$ is a solution of the subproblem\n",
    "$\\min_{f_j} -U_j(f_j)+(r_j^T \\lambda) f_j$\n",
    "\n",
    "algorithm\n",
    "- given\n",
    "- repeat\n",
    "- Sum link prices along each route. Calculate $\\Lambda_j = r_j^T \\lambda$.\n",
    "- Optimize flows (separately) using flow prices. $f_j=argmax(U_j(f_j)-\\Lambda_j f_j)$\n",
    "- Calculate link capacity margins: $s:=c- Rf$\n",
    "- update link prices $\\lambda: = (\\lambda - \\alpha_k s)_+$\n",
    "\n",
    "decentralized:\n",
    "- links only need to know the flows that pass through them\n",
    "- flows only need to know prices on link they pass through\n",
    "\n",
    "Generating feasible flow rates:\n",
    "- iterates can be (and often are) infeasible, i.e., $Rf not \\preceq c$ (but we do have $Rf \\preceq c$ in the limit)\n",
    "- define $\\eta_i = t_i/c_i = (Rf)_i /c_j$\n",
    "- $\\eta_i < 1$ means link $i$ is under capacity\n",
    "- $\\eta_i > 1$ means link $i$ is over capcacity\n",
    "- define $f^{feas}$ as \n",
    "\n",
    "Convergence\n",
    "- $n=10$ flows, $m=12$ links: 3 or 4 links per flow\n",
    "- link capacities chosen randomly, uniform on [0,1, 1]\n",
    "- $U_j(f_j) = \\log f_j$\n",
    "- optimal flow as a function of price: $\\bar{f}_j=argmax ...$\n",
    "- initial prices: $\\lambda = 1$\n",
    "- constant stepsize $\\alpha_k = 3$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d2774",
   "metadata": {},
   "source": [
    "## Methods for nonconvex optimization problems\n",
    "\n",
    "Convex optimization algorithms are in general\n",
    "- global (find global minimum)\n",
    "- fast (run in polynomial-time)\n",
    "\n",
    "For general nonconvex problems, we have to give up one\n",
    "- local optimization methods are fast, but may not find global minimum (and even when they do, cannot  certify it)\n",
    "- global optimization methods find global minimum (and certify it), but are often"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34091366",
   "metadata": {},
   "source": [
    "### Branch and bounds\n",
    "\n",
    "- methods for global optimization for nonconvex problems\n",
    "- basic idea\n",
    "- partition feasible set into convex sets, and find lower/upper bounds for each\n",
    "- maintain global lower and upper bounds: quit if they are close enough to each other\n",
    "- else, refine partitino and repeat\n",
    "- Often slow; exponential \n",
    "\n",
    "find global minimum of function $f: R^m \\to R$ over a $m$-dimensional rectangle $Q_{init}$, to within some preset accuracy $\\epsilon$\n",
    "\n",
    "algorithm sketch:\n",
    "- computer lower and upper bounds on $f^*$\n",
    "- partition (split) $Q_{init}$ into two rectangles $Q_{init}=Q_1\\cup Q_2$ \n",
    "- compute \n",
    "- update lower and upper bounds on $f^*$ ....\n",
    "- refine partition, by splitting $Q_1$ or $Q_2$, and repeat step 3 and 4\n",
    "\n",
    "### Local search\n",
    "A heuristic method to solve computationally hard problems\n",
    "- moves from solution to solution in the space of candidate solutions (i.e., search space) by applying local changes, until a solution deemed optimal is found or a bound on the number of steps /time taken is reached\n",
    "\n",
    "Problems that local search has been applied for\n",
    "- minimum vertex cover problem\n",
    "- TSP\n",
    "- Boolean SAT problem\n",
    "\n",
    "how to choose a good neighbourhoood for the problem\n",
    "- guided by intuition\n",
    "- very little theory avaiable as a guide\n",
    "\n",
    "which solution in the neighborhood to move to\n",
    "- decided using only information in the neighborhood\n",
    "- can get stuck in a local optimal point\n",
    "\n",
    "### Sequential convex programming (SCP)\n",
    "- A local optimization method for nonconvex problems based on solving a sequence of convex problems\n",
    "- SCP is a heuristic \n",
    "- SCP often works well, i.e., finds a feasible point with good, if not optimal, objective value\n",
    "\n",
    "Basic idea:\n",
    "`trust region`\n",
    "\n",
    "For differentiable functions\n",
    "- Use first-order Taylor expansion for the affine approximation\n",
    "- Use (convex part of) second-order Taylor expansion for the convex approximation\n",
    "\n",
    "### Affine and convex approximation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e8ef2f9fcac0817bca9a7ca376f64f20b4df5ea3bf7af756a50bda7d3557ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}