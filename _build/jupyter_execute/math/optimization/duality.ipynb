{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "947a934a",
   "metadata": {},
   "source": [
    "# SDP and Duality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f87e4",
   "metadata": {},
   "source": [
    "## Duality\n",
    "\n",
    "Optimization in standard form (not necessarily convex)\n",
    "- minimize $f_0(x)$\n",
    "- s.t. $f_i\\le 0$, $h_i(x)=0$\n",
    "\n",
    "### Lagrangian \n",
    "$L: R^n\\times R^m \\times R^p \\to R$ with $dom (L) \\ D \\times R^m \\times R^p$\n",
    "$L(x,\\lambda,\\nu)= f_0(x) + \\sum_i^m \\lambda_i f_i(x) + \\sum_i^p \\nu_i h_i$\n",
    "\n",
    "- Lagrange multipliers (Lagrangian dual variables): $\\lambda_i$, $\\nu_i$\n",
    "- augment $f_0(x)$ by weighted sum of contraint functions (penalty functions)\n",
    "\n",
    "Lagrange dual function: $g:R^m\\times R^p \\to R$,\n",
    "$$\n",
    "g(\\lambda,\\nu) = \\inf L(x,\\lambda,\\nu)\n",
    "$$\n",
    "\n",
    "Lagrange dual problem\n",
    "- $\\max g(\\lambda, \\nu)$\n",
    "- $\\lambda \\prec 0$\n",
    "\n",
    "### LP and its dual\n",
    "#### standard form LP\n",
    "- minimize $c^T x$\n",
    "- s.t. $Ax = b$, $x \\succeq 0$\n",
    "\n",
    "#### Lagrangian\n",
    "$L(x,\\lambda,\\nu) = c^T x + v^T (Ax-b) - \\lambda^T x = -b^T v + (c+A^T v- \\lambda)^T x$\n",
    "\n",
    "#### Lagrange dual function\n",
    "$g(\\lambda,\\nu) = \\inf_{x} L(x,\\lambda,\\nu)=-b^T v, A^Tv-\\lambda+c=0; -\\infty, otherwise$\n",
    "\n",
    "#### Dual problem\n",
    "- maximize $-b^T v$\n",
    "- s.t. $A^T v + c \\preceq 0$\n",
    "\n",
    "### Lagrange dual and conjugate function\n",
    "optimization with affine inequality and equality constraints\n",
    "- minimize $f_0(x)$\n",
    "- s.t. $Ax \\preceq b$, $C x = d$\n",
    "\n",
    "Lagrange dual function\n",
    "$g(\\lambda,v) = \\inf_x f_0(x) + (A^T \\lambda + C^T v)^T x - b^T \\lambda -d^T v=-f_0^*(-A^T \\lambda - C^T v) - b^T \\lambda -d^T v$\n",
    "\n",
    "- recall definition of conjugate $f^*(y)=\\sup_x (y^T x - f(x))$\n",
    "- simplifies derivation of dual if conjugate of $f_0$ is known\n",
    "$f_0(x) = \\sum_i^n x_i \\log x_i$, $f_0^*(y) = \\sum_i^n e^{y_i-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f371c44",
   "metadata": {},
   "source": [
    "````{prf:example} driving the dual problem\n",
    "- $\\min x_1^2 + x_2^2$\n",
    "- s.t $3x_1 + 5x_2 = 7$, $6x_1 + 4x_2 \\le 9$\n",
    "\n",
    "`Lagrangian`: $L(x,\\lambda,\\mu) = x_1^2 + x_2^2 + \\lambda(6x_1 + 4x_2-9)+\\mu(3x_1+5x_2 -7)$\n",
    "\n",
    "`Lagrange dual function`: \n",
    "$g(\\lambda,\\mu) = \\inf_{x\\in R^2} x_1^2 + x_2^2 + \\lambda(6x_1 + 4x_2 - 9) + \\mu (3x_1 + 5x_2 - 7) = -13 \\lambda^2 - 8.5 \\mu^2 - 19\\lambda \\mu -9\\lambda -7\\mu$\n",
    "\n",
    "`dual problem`: $\\max-13\\lambda^2 - 8.5 \\nu^2 ...$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45af6e",
   "metadata": {},
   "source": [
    "## Primal and Lagrange dual problems\n",
    "\n",
    "- Primal problem (convex or non-convex)\n",
    "\n",
    "- Lagrange dual problem (is a `convex` optimization problem)\n",
    "\n",
    "Property 1: $g(\\lambda ,\\nu)$ is concave on $(\\lambda,\\nu)$ even if the original problem is not convex\n",
    "\n",
    "Proof: For each fixed $x, g(\\lambda, \\nu)$ is ... pointwise infimum ... concave\n",
    "\n",
    "````{prf:example} Max-Cut\n",
    "<!-- NP-hard -->\n",
    "variable: \n",
    "- adjacency matrix of the graph $Q=1, \\{i,j\\}\\in E; 0, OW$\n",
    "- a cut decided by a vector $x\\in R^n$, $x_i = 1, i\\in S; -1, OW$\n",
    "- capacity of the cut $c(x) = \\frac{1}{4} \\sum_i^n\\sum_j^n (1-x_ix_j)Q_{ij}$ \n",
    "($1-x_ix_j=2$ if $\\{i,j\\}$ is in the cut set) $(x_i-x_j/2)^2$\n",
    "\n",
    "primal problem (NP Complete even if $Q\\succeq 0$):\n",
    "- minimize $x^T Q x$ \n",
    "- s.t. $x\\in \\{-1, 1\\} \\forall i=1, \\dots, n$\n",
    "\n",
    "The maximum cut is $c_\\max = \\frac{1}{4} \\sum_i\\sum_j Q_{ij} - \\frac{1}{4} p^*$\n",
    "\n",
    "- `Dual problem` (SDP)\n",
    "let $\\Lambda=diag(\\lambda_1,\\dots, \\lambda_n)$, \n",
    "the Lagrangian is $L(x,\\lambda) = x^T Q x - \\sum_i^n \\lambda_i (x_i^2 - 1) = x^T (Q-\\Lambda) x + Tr(A)$.\n",
    "the dual is \n",
    "- maximize $tr(\\Lambda)$\n",
    "- s.t. $Q-\\Lambda\\succeq 0$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac153ce",
   "metadata": {},
   "source": [
    "Property 2: $g(\\lambda,\\nu)\\le f_0(x)$ for every primal feasible x and dual feasible $(\\lambda,\\nu)$\n",
    "\n",
    "Remarks: \n",
    "- $f_0(x) - g(\\lambda,\\nu)$: `duality gap` for $(x,\\lambda,\\nu)$, which is always non-negative $p^*-d^*$: optimal duality gap\n",
    "- $d^*\\le p^*$: `weak duality`\n",
    "for any optimization problem: convex or not can be used to find a non-trivial lower bound on $p^*$, for difficult primal problems\n",
    "\n",
    "any dual feaisble solution gives a `lower bound` on primal objective value:\n",
    "for any feasible $x$ \n",
    "$x^T Qx \\ge x^T \\Lambda x = \\sum_i \\Lambda_{ii} x_i^2 = Tr(\\Lambda)$\n",
    "\n",
    "## Strong duality\n",
    "`strong duality:` $d^* = p^*$\n",
    "- does not hold in general\n",
    "- (usually) holds for convex problems\n",
    "- conditions that guarantee strong duality in convex problems are called constraint qualifications\n",
    "\n",
    "### Slater's constraint qualification\n",
    "strong duality holds for a convex problem\n",
    "- minimize $f_0(x)$\n",
    "- s.t. $f_i(x)\\le 0$, $Ax=b$\n",
    "\n",
    "if it is strictly feasible, i.e., \n",
    "$\\exists x \\in int(D): f_i(x) < 0, i=1,\\dots,m, Ax = b$.\n",
    "\n",
    "Can be refined: linear inequalities do not need to hold with strict inequality\n",
    "\n",
    "If an LP (in any form) has an optimal solution $x^*$, then the dual also has an optimal solution $y^*$ and $C^T x^* = b^T y^*$\n",
    "\n",
    "### Geometric interpretation\n",
    "interpretation of dual function: $g(\\lambda) = \\inf_{(u,t)\\in\\mathcal{G}} (t + \\lambda u)$, where $\\mathcal{G}=\\{(f_1(x), f_0(x)) \\}$\n",
    "- $\\lambda u + t = g(\\lambda)$ is (non-vertical) supporting hyperplane to $\\mathcal{G}$ which intersects t-axis at $t=g(\\lambda)$\n",
    "\n",
    "epigraph variation: same interpretation if $\\mathcal{G}$ is replaced with $\\mathcal{A}=\\{ (u,t) \\mid f_1(x)\\le u, f_0(x)\\le t \\text{ for some } x\\in D \\}$\n",
    "\n",
    "`strong duality`\n",
    "- holds if there is a non-vertical suporting hyperplane to $\\mathcal{A}$ at $(0,p^*)$\n",
    "- for convex problem, $\\mathcal{A}$ is convex, hence has supp. hyperplane at $(0,p^*)$ \n",
    "- Salter's condition: if there exist $(\\tilde{u},\\tilde{t})\\in\\mathcal{A}$ with $\\tilde{u}<0$, then supporting hyperplanes at $(0,p^*)$ must be non-vertical\n",
    "\n",
    "### Sensitivity interpretation\n",
    "(`unperturbed`) optimization problem and its dual\n",
    "- minimize $f_0(x)$\n",
    "- s.t. $f_i(x)\\le 0$, $h_i(x)=0$\n",
    "- dual: $\\max g(\\lambda,\\nu)$ s.t. $\\lambda \\succeq 0$\n",
    "\n",
    "`perturbed` problem and its dual\n",
    "- $\\min f_0(x)$\n",
    "- s.t. $f_i(x)\\le u_i$, $h_i(x) = v_i$\n",
    "- dual: $\\max g(\\lambda, \\nu) - u^T\\lambda - v^T \\nu$ s.t. $\\lambda \\succeq 0$\n",
    "\n",
    "- $p^*(u,v)$ is optimal value as a function of $u$, $v$ \n",
    "- `local sensitivity`: if strong duality holds and $p^*(u,v)$ is differentiable at $(0,0)$\n",
    "$\\lambda^*_i = - \\frac{\\partial p^*(0,0)}{\\partial u_i}$, $\\nu^*_i = - \\frac{\\partial p^*(0,0)}{\\partial v_i}$\n",
    "\n",
    "### Saddle-point interpretation\n",
    "Assume no equality constraints (results can be easily extended)\n",
    "$p^*=\\inf_x \\sup_{\\lambda\\succeq 0} L(x,\\lambda)$,\n",
    "$d^*=\\sup_{\\lambda\\succeq 0} \\inf_x L(x,\\lambda)$\n",
    "\n",
    "- weak duality: $\\sup_{\\lambda\\succeq 0} \\inf_x L(x,\\lambda) \\le \\inf_x \\sup_{\\lambda\\succeq 0} L(x,\\lambda)$\n",
    "- strong duality: $\\sup_{\\lambda\\succeq 0} \\inf_x L(x,\\lambda) = \\inf_x \\sup_{\\lambda\\succeq 0} L(x,\\lambda)$\n",
    "- `Max-min inequality` generally holds: $\\sup_{z\\in Z} \\inf_{w\\in W} f(w,z) \\le \\inf_{w\\in W} \\sup_{z\\in Z} f(w,z) \\forall f,W,Z$\n",
    "- Strong max-min property (`saddle point`)\n",
    "\n",
    "saddle-pint for $f$: a pair $\\tilde{w} \\in W$, $\\tilde{z}\\in Z$ that satisfy\n",
    "$f(\\tilde{w},z) \\le f(\\tilde{w},\\tilde{z})\\le f(w, \\tilde{z}), \\forall w\\in W, z\\in Z$\n",
    "$\\Longleftrightarrow f(\\tilde{w},\\tilde{z}) = \\inf_{w\\in W} f(w,\\tilde{z}), f(\\tilde{w},\\tilde{z}) = \\sup_{z\\in Z} f(\\tilde{w}, z)$\n",
    "$\\Longleftrightarrow strong max-min property holds$\n",
    "\n",
    "$x^*, \\lambda^*$ are primal and dual optimal points and strong duality holds iff $(x^*,\\lambda^*)$ form a saddle-point for the Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec9f76",
   "metadata": {},
   "source": [
    "### Game interpretation\n",
    "- player x choose strategy from $1,2,\\dots, n$\n",
    "- player y choose strategy from $1,2,\\dots, n$\n",
    "- $P_{ij}$ is the amount x pays to y (payoff) when $x$ plays strategy i, y plays startegy $j$\n",
    "\n",
    "mixed strategy:\n",
    "* $u_i$: prob (player x chooses strategy $i$)\n",
    "* $y_i$: prob (player y chooses strategy $i$)\n",
    "\n",
    "expected payoff: $u^T P v$\n",
    "\n",
    "Suppose $x$ fixes strategy $u$, then $y$ plays (decides $v$) to maximize expected payoff: \n",
    "```{math}\n",
    "\\max u^T P v s.t. \\sum_{i=1}^m v_i= 1, v \\succeeq 0\n",
    "```\n",
    "so x must choose u to minimize $\\max_i (P^T u)_i$:\n",
    "$\\min_u \\max_i (P^T u)_i$ s.t. $\\sum_i^n u_i = 1$ and $u\\succeq 0$\n",
    "equivalent to? \n",
    "$\\min t$ s.t. $P^T u \\preceq t 1$, $u^T 1 = 1$ and $u\\preceq 0$\n",
    "\n",
    "LP(1) with optimal value: $p_1^*$\n",
    "\n",
    "Suppose $y$ plays first (v given), then x choose u to minimize expected payoff:\n",
    "$\\min u^T P v$, s.t. $\\sum_i^n u_i =1$ and $u \\preceq 0$\n",
    "\n",
    "LP(2) with optimal value: $p_2^*$\n",
    "\n",
    "LP(1) and LP(2) are duals of each other and thus have the same optimal values: $p_1^* = p_2^*$\n",
    "\n",
    "Therefore, there is no advantage to play second ...\n",
    "\n",
    "Consider payoff function $f(u,v) = u^T P v$, the optimum $u^*$ for LP(1) and the optimum $v^*$ for LP(2) form a saddle-point for $f(u,v)$\n",
    "$f(u^*,v) \\le f(u^*,v^*) \\le f(u,v^*)$ equivalent to\n",
    "$f(u^*,v^*)=\\inf_u f(u,v^*)$ and $f(u^*,v^*) = \\sup_v f(u^*,v)$\n",
    "\n",
    "`Nash equilibrium` of the game: $(u^*,v^*)$ such that \n",
    "- $u^*$ is the best response of player x with respect to $v^*$\n",
    "- $v^*$ is the best response of player y with respect to $u^*$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab1d44",
   "metadata": {},
   "source": [
    "#### Usage example of duality \n",
    "`Duality gives a way to analytically solve an optimization problem`\n",
    "````{prf:example} \n",
    "$\\min ||x||_2^2$ s.t. $Ax = b$\n",
    "\n",
    "Lagrangian: $L(x,\\nu)=x^T x + \\nu^T (Ax - b)$\n",
    "Lagrange dual function: $g(v)=\\min x^T x + \\nu^T A x - \\nu^T b$\n",
    "```{math}\n",
    "\\frac{\\partial (x^T x + \\nu^T A x)}{\\partial x} = 2 x^T + \\nu^T A = 0\n",
    "\\implies x = -\\frac{A^T \\nu}{x}\n",
    "```\n",
    "then $g(\\nu)=-\\frac{1}{4} \\nu^T A A^T \\nu - \\nu^T b$\n",
    "\n",
    "Dual problem: $\\max_\\nu - \\frac{1}{4} \\nu^T A A^T \\nu - \\nu^T b$\n",
    "$\\mu = -2 (AA^T)^{-1} b$ and $x=A^T ...$\n",
    "````\n",
    "`derivative is the transpose of gradient`\n",
    "\n",
    "\n",
    "`Duality is useful to find non-trivial lower bounds to non-convex minimization problems`\n",
    "````{prf:example} \n",
    "$\\min x^T W x, W \\succeq$ s.t. $x_i \\in \\{ -1, 1 \\}$.\n",
    "\n",
    "Relaxation: $\\min x^T W x$ s.t. $-1 \\le x_i \\le 1$\n",
    "\n",
    "use duality t oobtain better lower bound (as on page15 of )\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced4b92",
   "metadata": {},
   "source": [
    "### Non-convex problem with strong duality\n",
    "\n",
    "strong duality not implies convex problem\n",
    "\n",
    "````{prf:example}  Non-convex problem with strong duality\n",
    "$\\min x^T A x$, $A \\in S^n$, s.t. $x^T x = 1$.\n",
    "\n",
    "solution: $\\min_{||x||_2 = 1} x^T A x = \\text{min eigval of } A $\n",
    "\n",
    "let $x=\\sum_i^n \\alpha_i v_i$ (linear combination of orthogonal eigenvectors of $A$)\n",
    "$v_i$ is an eigenvector corresponding to eigenvalue $\\lambda_i$: $A v_i = \\lambda_i v_i, ||v_i||_2 = 1$. \n",
    "we have $x^T A x = (\\sum_i \\alpha_i v_i)^T A (\\sum_i \\alpha_i v_i) = \\sum_i \\alpha_i^2 \\lambda_i ||v_i||_2^2 = \\sum_i^n \\alpha_i^2 \\lambda_i $\n",
    "since $x^T x = 1$, we know $\\sum_i \\alpha_i^2 = 1$.\n",
    "\n",
    "Therefore, $x^T A x$ is minimized when $\\alpha_j =1$ for $\\lambda_j = \\min(\\lambda_1,\\dots,\\lambda_j)= ...$\n",
    "\n",
    "Derive the dual problem\n",
    "- Lagrangian: $L(x,\\nu)=x^T A x + \\nu (x^T x - 1)$\n",
    "- Lagrangian dual function: $g(\\nu) = \\min_x L(x,\\nu)= \\min_x x^T (A+\\nu I) x - \\nu = -\\nu, \\text{if } A+\\nu I \\succeq 0; -\\infty, \\text{ otherwise } $\n",
    "- dual problem: $\\max -\\nu$ s.t. $A + \\nu I \\succeq 0$\n",
    "\n",
    "Therefore, the largest $-\\nu$ to make $A + \\nu I \\succeq 0$ is $\\min\\{\\lambda_1, \\dots, \\lambda_n\\}$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3d581",
   "metadata": {},
   "source": [
    "### Complementary slackness for LP\n",
    "\n",
    "`Primal`: $\\min c^T x$ s.t. $Ax \\succeq b$, $x\\succeq 0$\n",
    "\n",
    "`Dual`: $\\max b^T y$ s.t. $A^T y \\preceq c$, $y \\preceq 0$\n",
    "\n",
    "````{prf:theorem} Complementary slackness theorem for LP\n",
    "A pair of feasible solutions $x\\in R^n$ and $y\\in R^m$ for primal and dual LP problems is optimal iff\n",
    "$y_i (b_i - (Ax)_i)=0, \\forall i = 1, \\dots, m$\n",
    "$x_i (c_j - (A^T y)_j)=0, \\forall j = 1, \\dots, m$\n",
    "````\n",
    "Proof of sufficiency:\n",
    "Given $x, y$ satisfy complementary slackness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02695608",
   "metadata": {},
   "source": [
    "## Karush_Kuhn-Tucker (KKT) conditions\n",
    "\n",
    "the following four conditions are called KKT conditions (for a problem with differentiable $f_i$, $h_i$):\n",
    "$\\min f_0(x)$ s.t. $f_i(0)$\n",
    "\n",
    "- primal constraints: \n",
    "- dual constraints:\n",
    "- complementary slackness: $\\lambda_i f_i(x)=0$, $i=1,\\dots,m$\n",
    "- gradient of Lagrangian with respect to $x$ vanishes:\n",
    "```{math}\n",
    "\\nabla f_0(x) + \\sum_i^m \\lambda_i \\nabla f_i(x) + \\sum_i^p \\nu_i \\nabla h_i(x) = 0\n",
    "```\n",
    "\n",
    "For any optimization problem with differentiable objective and constraint functions, if strong duality holds, any pair of primal and dual optimal points $x^*, \\lambda^*, \\nu^*$ must satisfy the KKT conditions\n",
    "$g(\\lambda^*, \\nu^*) = \\inf_x L(x,\\lambda^*,\\nu^*) = \\inf_x f_0(x) + \\sum_i^m \\lambda_i^* f_i(x) + \\sum_i^p \\nu_i^* h_i(x) = f_0(x^*)$\n",
    "\n",
    "### KKT conditions for convex programs\n",
    "For a convex program, any points $\\tilde{x}$, $\\tilde{\\lambda}$, $\\tilde{\\nu}$ that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap \n",
    "$\\tilde{\\lambda_i} \\ge 0$, $\\forall i \\to L(x,\\tilde{\\lambda},\\tilde{\\nu})$ is convex in $x$.\n",
    "\n",
    "gradient of Lagrangian with respect to $\\tilde{x}$ vanishes $\\implies$ $\\tilde{x}$ minimizes $L(x,\\tilde{\\lambda}, \\tilde{\\nu})$,\n",
    "then $g(\\tilde{\\nu},\\tilde{\\nu})=\\inf_x L(x,\\tilde{\\lambda},\\tilde{\\nu}=...$\n",
    "since $g(\\tilde{\\lambda},$\n",
    "\n",
    "If a convex program with differentiable objective and constraint functions satisfies Salter's conditions, then KKT condition is necessary and sufficient for optimality\n",
    "- recall that Slater implies strong duality\n",
    "\n",
    "Significance of KKT\n",
    "- solve KKT to derive \n",
    "\n",
    "````{prf:example} \n",
    "$\\min x_1^2 + 2x_2^2$ s.t. $x_1 + x_2 \\ge 3$\n",
    "\n",
    "To solve this, try to find $(x^*,\\lambda^*)$ that satisfy the KKT conditions\n",
    "````\n",
    "\n",
    "````{prf:example} Water-filling \n",
    "$\\min \\min - \\sum_i^k \\log (1 + p_i/N_i)$ s.t. $\\sum_i^k p_i \\le P$, $p_i\\ge 0, \\forall i = 1,\\dots, k$\n",
    "\n",
    "the algorithm takes the intuition from KKT\n",
    "\n",
    "Generalization:\n",
    "maxmimally allocate resource to pi with the current smallest marginal utility, until all resource P is used up.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15787ae",
   "metadata": {},
   "source": [
    "- equivalent formulation of a problem can lead to very different duals\n",
    "- reformulating \n",
    "\n",
    "- dual\n",
    "- dual\n",
    "\n",
    "````{prf:example} \n",
    "`Introducing new variables and equality constraints`\n",
    "\n",
    "minimize $f_0(Ax + b)$\n",
    "- dual function is constant: $g=\\inf_x L(x) = \\inf_x f_0(Ax_b) = p^*$\n",
    "- we have strong duality, but dual is quire useless\n",
    "\n",
    "reformulated problem and its dual\n",
    "- $\\min f_0(y)$ s.t. $Ax + b - y = 0$\n",
    "- $b^T \\nu - f_0^*(\\nu)$ s.t. $\\ge$\n",
    "````\n",
    "````{prf:example} \n",
    "`Making explicit constraint implicit`\n",
    "\n",
    "LP with box constraints: primal and dual problem:\n",
    "- $\\min c^T x$ s.t. $Ax=b, -1\\preceq x \\preceq 1$\n",
    "- $\\max -b^T \\nu - 1^T \\lambda_1 - 1^T \\lambda_2$ s.t. $c+A^T \\nu + \\lambda_1 - \\lambda_2 =0, \\lambda_1 \\succeq 0$, $\\lambda_2 \\succeq 0$\n",
    "\n",
    "reformulation with box constraints made implicit:\n",
    "- $\\minimize f_0(x) = c^Tx; \\infty$ s.t. \n",
    "\n",
    "dual function\n",
    "$g(\\nu) = \\inf_{-1\\preceq x \\preceq 1} (c^T x + \\nu^T(Ax - b))=-b^T\\nu - \\norm{A^T \\nu + c}_1$\n",
    "````\n",
    "\n",
    "#### Partial Lagrangian relaxation\n",
    "One can take Lagrangian with respect to only a subset of the constraints, e.g., \n",
    "\n",
    "If convex problem and Slater's conditino is satisfied: \n",
    "$\\max_{\\lambda \\succeq 0} \\tilde{g}(\\lambda) = \\min_{f_i(x)\\le 0} f_0(x)$\n",
    "let $x^*$ be the primal optimal point, $(\\lambda^*_1, \\lambda^*_2)$ be dual optimal point for $g(\\lambda_1,\\lambda_2)$ and $\\lambda_1'$ be dual optimal point for $\\tilde{g}(\\lambda)$\n",
    "```{math}\n",
    "f_0(x^*) \\ge \\tilde{g}(\\lambda_1') \\ge \\tilde{g} (\\lambda_1^*) = \\min f_0 (x) + \\lambda_1^* f_1 (x) \\le \\min f_0(x) + \\lambda_1^* f_1(x) + \\lambda_2^* f_2(x) = g(\\lambda_1^*, \\lambda_2^*) = f_0(x^*)\n",
    "```\n",
    "The aobve property extensible to general case, when the partial Lagrangian is derived by relaxing \n",
    "````{prf:example} \n",
    "`Transform objective function`\n",
    "\n",
    "$\\min ||Ax-b||$\n",
    "\n",
    "reformulation $\\min 1/2 ||y||^2$ s.t. $y=Ax-b$ dual problem on page 257, textbook\n",
    "````\n",
    "\n",
    "#### Generalized inequality\n",
    "\n",
    "dual problem\n",
    "- $\\max g(\\lambda_1, \\dots. \\lambda_m, \\nu)$ s.t. $\\lambda_i \\succeq_{K_i^*} 0, i=1,\\dots,m$\n",
    "\n",
    "Properties:\n",
    "- lower bound property: if $\\lambda_i \\succeq_{K_i^*} 0$, then $g$\n",
    "- weak duality ...\n",
    "- strong duality ...\n",
    "- KKT\n",
    "\n",
    "````{prf:example} Semidefinite program\n",
    "`primal SDP` $(F_i,G\\in S^k)$:\n",
    "$\\min c^T x$ s.t. $x_1 F_1 + \\cdots + x_n F_n \\preceq G$\n",
    "- Lagrange multiplier is matrix $Z\\in S^k$, define $<A,B>=tr(AB)$\n",
    "- Lagrangian $L(x,Z) = c^T x + Tr(Z(x_1 F_1 + \\cdots + x_n F_n -G) )$ \n",
    "- dual function $g(Z)=\\inf_x L(x,Z)=-tr(GZ), tr(F_i Z) + c_i = 0, \\; i=1, \\dots ,n; -\\infty, otherwise$\n",
    "\n",
    "`dual problem`:\n",
    "- $\\max -tr(GZ)$ s.t $Z\\succeq 0$, $tr(F_i Z) + c_i = 0$\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e8ef2f9fcac0817bca9a7ca376f64f20b4df5ea3bf7af756a50bda7d3557ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}