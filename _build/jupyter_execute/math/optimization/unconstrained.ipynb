{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261427e0",
   "metadata": {},
   "source": [
    "# Uncontraied optimization\n",
    "\n",
    "minimize $f(x)$\n",
    "- $f$ convex, twice continuously differentiable (hence $dom f$ open)\n",
    "- we assume optimal value $p^*=\\inf_x f(x)$ is attained (and finite)\n",
    "solving the uncontrained optimization = find the solution to:\n",
    "$\\nabla f(x^*)=0$ $(n equations)\n",
    "\n",
    "usually need iterative algorithms:\n",
    "- produce a sequence of $x^{(k)}\\in dom(f), k=1,2,\\dots$ with $f(x^{(k)}) \\to \\min f(x)$ (i.e., $\\nabla f(x^{(k)}) $) \n",
    "- \n",
    "\n",
    "Descent methods\n",
    "- start with $x^{(0)}$\n",
    "- update $x^{(k+1)} = x^{(k)} + t^{(k)} \\Delta x^{(k)}$ s.t $f(x^{(k+1)}< f(x^{(k)})$\n",
    "- other notations: $x^+=x+ t \\Delta x$, $x:=x+t\\Delta x$\n",
    "- $\\Delta x$ is the step, or search direction; $t$ is the step size, or step length\n",
    "- from convexity, $f(x^+) < f(x)$ implies $\\nabla f(x)^T \\Delta x < 0$ (i.e., $\\Delta x$ is a descent direction)\n",
    "\n",
    "General descent method.\n",
    "- given a starting point $x\\in dom(f)$.\n",
    "- repeat \n",
    "1. determie a descent direction $\\Delta x$.\n",
    "2. line search\n",
    "3. Update: $x:=x+t \\Delta $\n",
    "\n",
    "Determine a descent direction\n",
    "- one approach is gradient descent $\\Delta x = -\\nabla f(x)$\n",
    "- choose a step size\n",
    "1. line search methods: exact line search $t=argmin_{t>0} f(x+t \\Delta x)$\n",
    "2. backtracking \n",
    "\n",
    "find the steepest descent direction - steepest descent method\n",
    "normalized steepest descent direction\n",
    "$\\Delta x_{nsd} = argmin \\{ \\nabla f(x)^T v \\mid ||v|| = 1 \\}$\n",
    "$\\Delta x = -\\nabla f(x)$ is the steepest descent direction with respect to Euclidean norm\n",
    "\n",
    "so performance of steepest descent similar to graident descent\n",
    "\n",
    "### Newton's method\n",
    "Newton's direction $\\Delta x_{nt} = - \\nabla^2 f(x)^{-1} \\nabla f(x)$\n",
    "\n",
    "One interpretation\n",
    "- $x+\\Delta x_{nt}$ minimizes second order approximation\n",
    "$f(x+v)=f(x) + \\nabla f(x)^T v + \\frac{1}{2} v^T \\nabla^2 f(x) v$\n",
    "\n",
    "`intuition`: $f$ twice differentiable, so this quadratic model is very accurate when $x$ is near $x^*$.\n",
    "\n",
    "Newton's descrement (stopping criteria):\n",
    "$\\lambda(x) = (\\nabla f(x)^T \\nabla^2 f(x)^{(-1)} \\nabla f(x))^{1/2}$ a measure of the proximity of $x$ to $x^*$\n",
    "- gives an estimate of $f(x) - p^*$, using quadratic approximation $\\hat{f}$:\n",
    "$f(x) - \\inf_v \\hat{f} (x+v) = \\frac{1}{2} \\lambda(x)^2$\n",
    "- directional derivative in the Newton direction: $\\nabla f(x)^T \\Delta x_{nt} = -\\lambda(x)^2$\n",
    "\n",
    "(damped or guarded) Newton's method:\n",
    "\n",
    "given a starting point , tolerance $\\epsilon$\n",
    "repeat \n",
    "1. computer the Newton step and decrement.\n",
    "2. stopping criterion.\n",
    "3. linea search\n",
    "4. update\n",
    "\n",
    "- damped Newton phase: $t<1$\n",
    "- quadratically convergent phase: $t=1$\n",
    "- pros: convergence is rapid in general and quadratic near $x^*$\n",
    "- cons: compute and store $\\nabla^2 f(x)$ and $\\nabla^2f(x)^{-1}$ (cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e8ef2f9fcac0817bca9a7ca376f64f20b4df5ea3bf7af756a50bda7d3557ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}