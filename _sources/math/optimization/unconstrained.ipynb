{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncontraied optimization\n",
    "\n",
    "minimize $f(x)$\n",
    "- $f$ convex, twice continuously differentiable (hence $dom f$ open)\n",
    "- we assume optimal value $p^*=\\inf_x f(x)$ is attained (and finite)\n",
    "solving the uncontrained optimization = find the solution to:\n",
    "$\\nabla f(x^*)=0$ $(n equations)\n",
    "\n",
    "usually need iterative algorithms:\n",
    "- produce a sequence of $x^{(k)}\\in dom(f), k=1,2,\\dots$ with $f(x^{(k)}) \\to \\min f(x)$ (i.e., $\\nabla f(x^{(k)}) $) \n",
    "- \n",
    "\n",
    "Descent methods\n",
    "- start with $x^{(0)}$\n",
    "- update $x^{(k+1)} = x^{(k)} + t^{(k)} \\Delta x^{(k)}$ s.t $f(x^{(k+1)}< f(x^{(k)})$\n",
    "- other notations: $x^+=x+ t \\Delta x$, $x:=x+t\\Delta x$\n",
    "- $\\Delta x$ is the step, or search direction; $t$ is the step size, or step length\n",
    "- from convexity, $f(x^+) < f(x)$ implies $\\nabla f(x)^T \\Delta x < 0$ (i.e., $\\Delta x$ is a descent direction)\n",
    "\n",
    "General descent method.\n",
    "- given a starting point $x\\in dom(f)$.\n",
    "- repeat \n",
    "1. determie a descent direction $\\Delta x$.\n",
    "2. line search\n",
    "3. Update: $x:=x+t \\Delta $\n",
    "\n",
    "Determine a descent direction\n",
    "- one approach is gradient descent $\\Delta x = -\\nabla f(x)$\n",
    "- choose a step size\n",
    "1. line search methods: exact line search $t=argmin_{t>0} f(x+t \\Delta x)$\n",
    "2. backtracking \n",
    "\n",
    "find the steepest descent direction - steepest descent method\n",
    "normalized steepest descent direction\n",
    "$\\Delta x_{nsd} = argmin \\{ \\nabla f(x)^T v \\mid ||v|| = 1 \\}$\n",
    "$\\Delta x = -\\nabla f(x)$ is the steepest descent direction with respect to Euclidean norm\n",
    "\n",
    "so performance of steepest descent similar to graident descent\n",
    "\n",
    "### Newton's method\n",
    "Newton's direction $\\Delta x_{nt} = - \\nabla^2 f(x)^{-1} \\nabla f(x)$\n",
    "\n",
    "One interpretation\n",
    "- $x+\\Delta x_{nt}$ minimizes second order approximation\n",
    "$f(x+v)=f(x) + \\nabla f(x)^T v + \\frac{1}{2} v^T \\nabla^2 f(x) v$\n",
    "\n",
    "`intuition`: $f$ twice differentiable, so this quadratic model is very accurate when $x$ is near $x^*$.\n",
    "\n",
    "Newton's descrement (stopping criteria):\n",
    "$\\lambda(x) = (\\nabla f(x)^T \\nabla^2 f(x)^{(-1)} \\nabla f(x))^{1/2}$ a measure of the proximity of $x$ to $x^*$\n",
    "- gives an estimate of $f(x) - p^*$, using quadratic approximation $\\hat{f}$:\n",
    "$f(x) - \\inf_v \\hat{f} (x+v) = \\frac{1}{2} \\lambda(x)^2$\n",
    "- directional derivative in the Newton direction: $\\nabla f(x)^T \\Delta x_{nt} = -\\lambda(x)^2$\n",
    "\n",
    "(damped or guarded) Newton's method:\n",
    "\n",
    "given a starting point , tolerance $\\epsilon$\n",
    "repeat \n",
    "1. computer the Newton step and decrement.\n",
    "2. stopping criterion.\n",
    "3. linea search\n",
    "4. update\n",
    "\n",
    "- damped Newton phase: $t<1$\n",
    "- quadratically convergent phase: $t=1$\n",
    "- pros: convergence is rapid in general and quadratic near $x^*$\n",
    "- cons: compute and store $\\nabla^2 f(x)$ and $\\nabla^2f(x)^{-1}$ (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing one lecture [todo]\n",
    "\n",
    "### infeasible start Newton method\n",
    "\n",
    "A generalization that deals with infeasible initial points and iterates\n",
    "- let x be a point that we do not assume to be feasible\n",
    "- find $x+\\Delta x_{nt}$ that solves the second-order approximation\n",
    "$\\min \\hat{f}(x+v)=f(x)+\\nabla f(x)^T v + (1/2)v^T\\nabla^2 f(x) v$ s.t. $A(x+v) =b$\n",
    "\n",
    "primal Newton step, dual Newton step\n",
    "\n",
    "primal-dual Newton step (an alternative way to derive)\n",
    "- write optimality condition as $r(y) = 0$, where \n",
    "$y=(x,v)$, $r(y) = (\\nabla f(x) + A^T v, Ax - b )$ can be understood as $(r_{dual} (x,v) , r_{pri} (x,v)$\n",
    "- linearizing $r(y) = 0$ gives $r(y+ \\Delta y) \\approx r(y) + D r(y) \\Delta y = 0$\n",
    "`first-order approx of first-order approx = second-order`\n",
    "\n",
    "\n",
    "- given starting point $x\\in dom f, v$, tolerance $\\epsilon > 0$, $\\alpha\\in (0, 1/2)$, $\\beta \\in (0,1)$.\n",
    "- repeat\n",
    "1. Compute primal aand dual Newton steps $\\Delta x_{nt}$, $\\Delta v_{nt}$\n",
    "2. Backtracking line search on $||r||_2$. $t:=1$.\n",
    "while $||r(x+t\\Delta x_{nt}, v+t \\Delta v_{nt})||_2 > (1-\\alpha t) || r(x,v)||_2$, $t:=\\beta t$\n",
    "3. Update. $x:=x+t\\Delta x_{nt}$, $v:=v+ t\\Delta v_{nt}$.\n",
    "- until $Ax = b$ and $||r(x,v) || \\le \\epsilon$.\n",
    "\n",
    "$r(y) + D r(y) \\Delta y = 0$\n",
    "\n",
    "- not a decent method: \n",
    "- the norm of $r$ decreases in the Newton's direction:\n",
    "- if $t=1$, the next iterate will be feasible, and all the following iterates will be feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for Equality Constrained Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interior-Point methods\n",
    "\n",
    "### basic ideas\n",
    "\n",
    "move inequality constraint to objective function via `indicator functions`\n",
    "- $\\min f_0(x) + \\sum_{i=1}^m I_- (f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "where $I_-(u)=0$ if $u\\le 0$, $I_-(u)=\\infty$ otherwise (indicator function of $R_-$).\n",
    "\n",
    "#### Logarithmic barrier function\n",
    "\n",
    "approximation via `logarithmic barrier`: fix some $t>0$\n",
    "- $\\min f_0(x) - (1/t) \\sum_{i=1}^m \\log (-f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "$\\phi(x)=-\\sum_{i=1}^m \\log(-f_i(x))$, $dom \\phi = \\{x\\mid f_1(x) \\le 0, \\dots, f_m(x)<0 \\}$\n",
    "- convex (follows from composition rules)\n",
    "- twice continuously differentiable, with `gradient Hessian`\n",
    "\n",
    "becomes\n",
    "- $\\min f_0(x) - (1/t)\\sum \\log(-f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "- difficult to minimize using Newton's method (from a random starting point) when t is large\n",
    "\n",
    "because Hessian varies rapidly near boundary of feasibility set\n",
    "\n",
    "- can be circumvented by solving a sequence of problems with increasing $t$\n",
    "\n",
    "startnig each Newton minmization from the solution to the problem with previous $t$\n",
    "\n",
    "#### Central path\n",
    "\n",
    "- central paths: $\\{ x^*(t) \\mid t>0 \\}$\n",
    "- $x^*(t)$: central points\n",
    "\n",
    "example: central path for an LP\n",
    "- $\\min c^T x$\n",
    "- s.t. $a_i^T x \\le b_i$, $i=1,\\dots,6$\n",
    "\n",
    "hyperplane $c^T x = c^T x^*(t)$ is tangent to level curve of $\\phi$ through $x^*(t)$\n",
    "\n",
    "* take the central path through interior of the feasible set\n",
    "\n",
    "`analytic center` of a set of convex inequalities and linear equations\n",
    "$f_i(x) \\le 0$, $i=1,\\dots,m$, $Fx=g$\n",
    "is defined as the optimal point of \n",
    "- $\\min -\\sum_i^m \\log(-f_i(x))$\n",
    "- s.t. $Fx=g$\n",
    "analytic center of linear inequalities $a_i^T x\\le b_i$, $i=1,\\dots,m$\n",
    "\n",
    "$x_{ac}$ is minimizer of $\\phi(x) = -\\sum_i^m \\log(b_i-a_i^T x)$\n",
    "\n",
    "### Barrier method (one interior-point method)\n",
    "\n",
    "- given strictly feasible $x$, $t:=t^{(0)}>0$, $\\mu>1$, tolerance $\\epsilon > 0$.\n",
    "repeat \n",
    "1. Centering step. Compute $x^*(t)$ by minimizing $t f_0 + \\phi$, subject to $Ax=b$\n",
    "2. Update $x:=x^*(t)$\n",
    "3. Stopping criterion. quit if \n",
    "4. increase $t$. $t:=\\mu t$\n",
    "\n",
    "choice of $\\mu$ involves a trade-off: large $\\mu$ means fewer outer interations, more inner (Newton) iterations; typical values: $\\mu=10-20$\n",
    "(for more practical choices of parameters, pp.570, textbook)\n",
    "\n",
    "#### dual points from central path\n",
    "\n",
    "every $x^*(t)$ corresponds to a dual feasible point (`of the original inequality constrained problem`)\n",
    "$\\lambda_i^*(t) = 1/(-tf_i(x^*(t)))$ and $\\nu^*(t) = w/t$\n",
    "\n",
    "verification:\n",
    "$x^*(t)$ solves \n",
    "- $\\min t f_0(x) + \\phi(x)$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "implies \n",
    "- $Ax^* = b$, $f_i(x^*) < 0$, $i=1, \\dots, m$\n",
    "- $\\exists w$, $t\\nabla f_0(x^*) + \\sum \\frac{1}{-f_i(x^*)} \\nabla f_i(x^*) + A^T w=0$\n",
    "\n",
    "implies \n",
    "- $x^*(t)$ minimizes the Lagrangian (of the `original problem`)\n",
    "- $L(x,\\lambda^*(t), \\nu^*(t))=f_0(x) + \\sum \\lambda_i^*(t) f_i(x) + \\nu^*(t)^T (Ax-b)$ \n",
    "- at: $\\lambda_i^*(t) = 1/(-t f_i(x^*(t)))$ and $\\nu^*(t) = w/t$\n",
    "\n",
    "Duality gap $m/t$: \n",
    "$f_0(x^*(t)) \\ge p^* \\ge d^* \\ge g(\\lambda^*(t), \\nu^*(t)) = L(x^*(t), \\lambda^*(t),\\nu^*(t))=f_0(x^*(t))-m/t$\n",
    "\n",
    "#### Interpretation via KKT condition\n",
    "$x=x^*(t)$, $\\lambda=\\lambda^*(t)$, $\\nu=\\nu^*(t)$ satsify\n",
    "1. primal constraints: $f_i(x)\\le 0$, $i=1,\\dots,m$, $Ax=b$\n",
    "2. dual constraints: $\\lambda\\succeq 0$\n",
    "3. approximate complementary slackness: \n",
    "4. \n",
    "\n",
    "#### Convergnce \n",
    "The number of steps to converge within tolerance $\\epsilon$:\n",
    "\n",
    "plus the initial centering step (to compute $x^*(t^{(0)})$)\n",
    "\n",
    "Example: geometric program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e8ef2f9fcac0817bca9a7ca376f64f20b4df5ea3bf7af756a50bda7d3557ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
