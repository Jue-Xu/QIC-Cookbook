{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interior-Point methods\n",
    "\n",
    "### basic ideas\n",
    "\n",
    "move inequality constraint to objective function via `indicator functions`\n",
    "- $\\min f_0(x) + \\sum_{i=1}^m I_- (f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "where $I_-(u)=0$ if $u\\le 0$, $I_-(u)=\\infty$ otherwise (indicator function of $R_-$).\n",
    "\n",
    "#### Logarithmic barrier function\n",
    "\n",
    "approximation via `logarithmic barrier`: fix some $t>0$\n",
    "- $\\min f_0(x) - (1/t) \\sum_{i=1}^m \\log (-f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "$\\phi(x)=-\\sum_{i=1}^m \\log(-f_i(x))$, $dom \\phi = \\{x\\mid f_1(x) \\le 0, \\dots, f_m(x)<0 \\}$\n",
    "- convex (follows from composition rules)\n",
    "- twice continuously differentiable, with `gradient Hessian`\n",
    "\n",
    "becomes\n",
    "- $\\min f_0(x) - (1/t)\\sum \\log(-f_i(x))$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "- difficult to minimize using Newton's method (from a random starting point) when t is large\n",
    "\n",
    "because Hessian varies rapidly near boundary of feasibility set\n",
    "\n",
    "- can be circumvented by solving a sequence of problems with increasing $t$\n",
    "\n",
    "startnig each Newton minmization from the solution to the problem with previous $t$\n",
    "\n",
    "#### Central path\n",
    "\n",
    "- central paths: $\\{ x^*(t) \\mid t>0 \\}$\n",
    "- $x^*(t)$: central points\n",
    "\n",
    "example: central path for an LP\n",
    "- $\\min c^T x$\n",
    "- s.t. $a_i^T x \\le b_i$, $i=1,\\dots,6$\n",
    "\n",
    "hyperplane $c^T x = c^T x^*(t)$ is tangent to level curve of $\\phi$ through $x^*(t)$\n",
    "\n",
    "* take the central path through interior of the feasible set\n",
    "\n",
    "`analytic center` of a set of convex inequalities and linear equations\n",
    "$f_i(x) \\le 0$, $i=1,\\dots,m$, $Fx=g$\n",
    "is defined as the optimal point of \n",
    "- $\\min -\\sum_i^m \\log(-f_i(x))$\n",
    "- s.t. $Fx=g$\n",
    "analytic center of linear inequalities $a_i^T x\\le b_i$, $i=1,\\dots,m$\n",
    "\n",
    "$x_{ac}$ is minimizer of $\\phi(x) = -\\sum_i^m \\log(b_i-a_i^T x)$\n",
    "\n",
    "### Barrier method (one interior-point method)\n",
    "\n",
    "- given strictly feasible $x$, $t:=t^{(0)}>0$, $\\mu>1$, tolerance $\\epsilon > 0$.\n",
    "repeat \n",
    "1. Centering step. Compute $x^*(t)$ by minimizing $t f_0 + \\phi$, subject to $Ax=b$\n",
    "2. Update $x:=x^*(t)$\n",
    "3. Stopping criterion. quit if \n",
    "4. increase $t$. $t:=\\mu t$\n",
    "\n",
    "choice of $\\mu$ involves a trade-off: large $\\mu$ means fewer outer interations, more inner (Newton) iterations; typical values: $\\mu=10-20$\n",
    "(for more practical choices of parameters, pp.570, textbook)\n",
    "\n",
    "#### dual points from central path\n",
    "\n",
    "every $x^*(t)$ corresponds to a dual feasible point (`of the original inequality constrained problem`)\n",
    "$\\lambda_i^*(t) = 1/(-tf_i(x^*(t)))$ and $\\nu^*(t) = w/t$\n",
    "\n",
    "verification:\n",
    "$x^*(t)$ solves \n",
    "- $\\min t f_0(x) + \\phi(x)$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "implies \n",
    "- $Ax^* = b$, $f_i(x^*) < 0$, $i=1, \\dots, m$\n",
    "- $\\exists w$, $t\\nabla f_0(x^*) + \\sum \\frac{1}{-f_i(x^*)} \\nabla f_i(x^*) + A^T w=0$\n",
    "\n",
    "implies \n",
    "- $x^*(t)$ minimizes the Lagrangian (of the `original problem`)\n",
    "- $L(x,\\lambda^*(t), \\nu^*(t))=f_0(x) + \\sum \\lambda_i^*(t) f_i(x) + \\nu^*(t)^T (Ax-b)$ \n",
    "- at: $\\lambda_i^*(t) = 1/(-t f_i(x^*(t)))$ and $\\nu^*(t) = w/t$\n",
    "\n",
    "Duality gap $m/t$: \n",
    "$f_0(x^*(t)) \\ge p^* \\ge d^* \\ge g(\\lambda^*(t), \\nu^*(t)) = L(x^*(t), \\lambda^*(t),\\nu^*(t))=f_0(x^*(t))-m/t$\n",
    "\n",
    "#### Interpretation via KKT condition\n",
    "$x=x^*(t)$, $\\lambda=\\lambda^*(t)$, $\\nu=\\nu^*(t)$ satsify\n",
    "1. primal constraints: $f_i(x)\\le 0$, $i=1,\\dots,m$, $Ax=b$\n",
    "2. dual constraints: $\\lambda\\succeq 0$\n",
    "3. approximate complementary slackness: \n",
    "4. \n",
    "\n",
    "#### Convergnce \n",
    "The number of steps to converge within tolerance $\\epsilon$:\n",
    "\n",
    "plus the initial centering step (to compute $x^*(t^{(0)})$)\n",
    "\n",
    "Example: geometric program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic phase I method\n",
    "\n",
    "- $\\min (over x,s) s$\n",
    "- s.t. $f_i(x)\\le s, i=1,\\dots,m$\n",
    "- $Ax=b$\n",
    "\n",
    "* if $x$, $s$ feasible, with $s<0$, then $x$ is strictly feasible for (1)\n",
    "* if optimal value $p^*$ of (2) is positive, then problem (1) is feasible\n",
    "* if $p^* =0$  and attained, then problem (1) is feasible (but not strictly);\n",
    "if $p^* =0$ and not attained, then problem (1) is infeasible\n",
    "\n",
    "Sum of infeasibilities phase I method\n",
    "- $\\min 1^T s$\n",
    "- s.t. $s\\succeq 0$, $f_i(x)\\le s_i$, $i=1,\\dots,m$\n",
    "- $Ax=b$\n",
    "\n",
    "* The optimal value of (3) is zero and achieved iff problem (1) is feasible\n",
    "\n",
    "#### Phase I via Newton method\n",
    "\n",
    "#### Primal-dual interior-point method\n",
    "Update both primal and dual variables in the Newton method\n",
    "to solve the modified KKT conditions\n",
    "\n",
    "- no distinction between inner and outer iterations\n",
    "- the primal iterates are not necessarily feasible \n",
    "- $\\hat{\\eta}(x,\\lambda) = -f(x)^T \\lambda$: surrogate duality gap\n",
    "- backtracking line search based on the norm of the residuals \n",
    "\n",
    "#### backtracking line search\n",
    "based on the norm of the residuals, modified to ensure $f(x)\\prec 0$, $\\lambda \\succeq 0$.\n",
    "\n",
    "compute the largest positive step length that does not exceed one and gives $\\lambda^+\\succeq 0$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgradient Method\n",
    "\n",
    "$g$ is a subgradient of $f$ (not necessarily convex) at $x$ if\n",
    "$f(y)\\ge f(x) + g^T (y-x) \\forall y$\n",
    "\n",
    "- if $f$ is convex and differentiable, $\\nabla f(x)$\n",
    "- \n",
    "- \n",
    "\n",
    "````{prf:example} subgradient\n",
    ":label: subgradient\n",
    "\n",
    "$f=\\max\\{f_1,f_2\\}$ with $f_1$, $f_2$ convex and differentiable\n",
    "\n",
    "- $f_1(x_0)> f_2(x_0)$: unique subgradient $g=\\nabla f_1(x_0)$\n",
    "- $f_2(x_0)> f_1(x_0)$: unique subgradient $g=\\nabla f_2(x_0)$\n",
    "- $f_1(x_0) = f_2(x_0)$: subgradient form a line segment $[\\nablaf_1(x_0),\\nabla f_2(x_0)]$\n",
    "````\n",
    "\n",
    "### Subdifferential\n",
    "\n",
    "- set of all subgradients of $f$ at $x$ is called the subdifferential of $f$ at $x$, denoted $\\partial f(x)$\n",
    "- $\\partial f(x)$ is a closed convex set (can be empty)\n",
    "\n",
    "if $f$ is convex,\n",
    "- $\\partial f(x)$ is nonempty, for $x\\in dom f$\n",
    "- $\\partial f(x) = \\{ \\nabla f(x) \\}$, if $f$ is differentiable at $x$\n",
    "- if $\\partial f(x) = \\{ g \\}$, ...\n",
    "\n",
    "recall for $f$ convex, differentiable $f(x^*)=\\inf_x f(x) \\Longleftrightarrow 0 = \\nabla f(x^*)$\n",
    "\n",
    "### Optimality condition for unconstrained problem\n",
    "\n",
    "- $\\min f_0(x)$\n",
    "- s.t. $f_i(x) \\le 0, i\\in [m$\n",
    "\n",
    "we assume \n",
    "- $f_i$ convex, defined on $R^n$ (hence subdiffereentiable)\n",
    "- strict feaisbility (Slater's condition)\n",
    "\n",
    "$x^*$ is primal optimal ($\\lambda^*$ is dual optimal) iff\n",
    "- $f_i(x^*)\\le 0$, $\\lambda^*_1 \\le 0$\n",
    "- ..\n",
    "- ..\n",
    "\n",
    "the algorithm is similar to ...\n",
    "\n",
    "#### Step size\n",
    "step size rules (step sizes are fixed before algorithm execution)\n",
    "- constant stetp size \n",
    "- constant step length\n",
    "- square summable\n",
    "- \n",
    "\n",
    "#### Convergence\n",
    "assumption\n",
    "- $f^*= \\inf_x f(x) > -\\infty$ , with $f(x^*)=f^*$\n",
    "- $||g||_2 \\le G \\forall g\\in \\partial f$\n",
    "- $R\\ge ||x^{(1)} - x^*||_2$\n",
    "\n",
    "convergence results: define $\\bar{f}=\\lim_{k\\to \\infty} f_{best}^{(k)}$\n",
    "- constant step size: $\\bar{h} - f^* \\le G^2 \\alpha/2$, i.e., converges to $G^2\\alpha/2$-suboptimal (converges to $f^*$ if $\\alpha$ small enough)\n",
    "- constant step length: $G\\gamma /2$-suboptimal\n",
    "\n",
    "````{prf:example}\n",
    "minimize $f(x) = \\max_{i=1,\\dots,m} (a_i^T x + b_i)$\n",
    "````\n",
    "\n",
    "### Subgradient method for constrained problems\n",
    "\n",
    "#### Projection\n",
    "projection: $s=argmin_{s\\in C} ||x-s||_s$\n",
    "\n",
    "example: linear equality constrained problem\n",
    "- $\\min f(x)$\n",
    "- s.t. $Ax=b$\n",
    "\n",
    "projection of $z$ onto $\\{ x\\mid Ax=b \\}$ is \n",
    "```{math}\n",
    "P(z) = z- A^T(AA^T)^{-1} (Az-b) \n",
    "= (I - A^T (AA^T)^{-1} A) z + A^T(AA^T)^{-1}b)\n",
    "```\n",
    "\n",
    "````{prf:example} Linear equality constrained problem\n",
    "- minimize $||x||_1$\n",
    "- s.t. $Ax = b$\n",
    "\n",
    "subgradient of objective is $g= sign(x)$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected subgradient method for dual problem\n",
    "\n",
    "(convex) primal: (Slater's condition holds)\n",
    "- minimize $f_0(x)$\n",
    "- s.t. $f_i(x) \\le 0 $\n",
    "\n",
    "solve dual problem \n",
    "- maximize $g(\\lambda)$\n",
    "- s.t. $\\lambda \\succeq 0$\n",
    "\n",
    "via projected subgradient method:\n",
    "$\\lambda^{(k+1)} =... $\n",
    "\n",
    "Given a starting point $\\lambda$....\n",
    "Repeat\n",
    "\n",
    "iterpretation: \n",
    "- $\\lambda_i$ is price for resource $f_i(x)$\n",
    "- price update $\\lambda_i^{(k+1)}=\\qty(\\lambda_i^{(k)}+\\alpha_k f_i(x^{(k)}))_+$\n",
    "- increase price $\\lambda_i$ if resource $i$ is over-utilized (i.e., $f_i(x) >0$)\n",
    "- decrease price $\\lambda_i$ if ... is under-utilizedo\n",
    "- not negative\n",
    "\n",
    "````{prf:example} \n",
    "minimize strictly convex quadratic $(P\\succ 0)$ over unit box:\n",
    "\n",
    "- minimize $(1/2)x^T P x -q^T x$\n",
    "- s.t. $x_i^2 \\le 1$\n",
    "\n",
    "- $L(x,\\lambda) = (1/2)x^T (P+diag(2\\lambda))x - q^T x - 1^T \\lambda$\n",
    "- $x^*(\\lambda) = (P+diag(2\\lambda))^{-1} q$\n",
    "- projected \n",
    "````\n",
    "\n",
    "#### Cutting plane oracle\n",
    "Oracle: a device that can answer question for us; we make no assumption on how an answer is found by the device\n",
    "\n",
    "Cutting plane oracle (also called separation oracle), once queried at $x$, either\n",
    "- asserts $x\\in X$ ($X\\subseteq R^n$)\n",
    "- or return a separating hyperplane between $x$ and $X$: $a\\neq 0$,\n",
    "$a^T z \\le b$ for $z\\in X$, $a^T x \\ge b$\n",
    "$(a,b)$ called a cutting-plane, or cut, since it eliminates the halfspace $\\{ z\\mid a^T z > b \\}$ from our search for a point in $X$\n",
    "\n",
    "- neutral cut\n",
    "- deep cut\n",
    "\n",
    "unconstrained optimization\n",
    "- $f_0:R^n\\to R$, convex; $x^*$ is the optimal solution; $X$ is the set of optimal points\n",
    "- Given $x$, find $g\\in \\partial f_0 (x)$\n",
    "- $x^*$ belongs to the halfspace: $\\{ z\\midg^T(z-x) \\le 0 \\},\\forall x$\n",
    "- $g^T(z-x)\\le 0$ defines a `cutting plane` at $x (a=g,b=g^Tx)$\n",
    "\n",
    "#### inequality constrained optimization\n",
    "- If $x$ is feasible, we have a (neural) objective cut\n",
    "$g_0^T(z-x)\\le 0$, $g_0\\in\\partial f_0(x)$\n",
    "- If $x$ is not feasible, e.g., $f_j(x)>0$, we have a (deep) feasibility cut\n",
    "$f_j(x) + g_j^T (z-x) \\le 0$, $g_j \\in \\partiaal f_j(x)$\n",
    "\n",
    "basic (conceptual) cutting-plane/localzation algorithm\n",
    "\n",
    "#### choosing the center point (specific localization methods)\n",
    "- center of gravity\n",
    "- analytic centering\n",
    "- Chebyshev center\n",
    "- Center of the maximum volume epplisoid (MVE)\n",
    "\n",
    "#### Bisection method on R\n",
    "- minimize convex $f:R\\to R$\n",
    "- $P_k$ is an interval\n",
    "- obvious choice for query point: $x^{(k+1)}:=midpoint(P_k)$\n",
    "\n",
    "Bisection algorithm for one-dimensional search\n",
    "\n",
    "given an initial interval $[l,u]$ known to contain $x^*$; a required tolerance $r>0$\n",
    "repeat \n",
    "    - $x:=(l+u)/2$.\n",
    "    - query the oracle at $x$.\n",
    "    - if the oracle determines that $x^*\\le x, u:=x$\n",
    "    - if the oracle determines that $x^*\\ge x, l:=x$\n",
    "until $u-l\\e 2r$\n",
    "\n",
    "for differentiable $f$: evaluate $f'(x)$ if $f'(x)<0$, $l:=x$; else $u:=x$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e8ef2f9fcac0817bca9a7ca376f64f20b4df5ea3bf7af756a50bda7d3557ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
